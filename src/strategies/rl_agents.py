"""
Reinforcement Learning Agents for Adaptive Trading
Market making, execution, and adaptive strategy agents using RL
"""

from typing import Dict, Any, Optional, Tuple, List
import numpy as np
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
from loguru import logger


class TradingEnvironment(gym.Env):
    """
    Base trading environment for RL agents
    
    State: [inventory, volatility, spread, OFI, time_remaining, position_pnl]
    Action: [bid_offset, ask_offset, cancel_flag]
    Reward: PnL - lambda * riskÂ²
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__()
        self.config = config or {}
        
        # State space: [inventory, volatility, spread, OFI, time, pnl]
        self.observation_space = spaces.Box(
            low=np.array([-1000, 0, 0, -1, 0, -10000]),
            high=np.array([1000, 1, 1, 1, 1, 10000]),
            dtype=np.float32
        )
        
        # Action space: [bid_offset, ask_offset, aggressiveness]
        self.action_space = spaces.Box(
            low=np.array([-0.01, -0.01, 0]),
            high=np.array([0.01, 0.01, 1]),
            dtype=np.float32
        )
        
        # Environment state
        self.inventory = 0
        self.cash = 0
        self.position_pnl = 0
        self.mid_price = 100.0
        self.spread = 0.01
        self.volatility = 0.01
        self.ofi = 0.0
        self.time_step = 0
        self.max_steps = 390  # Trading day
        
        # Risk parameters
        self.risk_aversion = config.get('risk_aversion', 0.01)
        self.inventory_penalty = config.get('inventory_penalty', 0.001)
    
    def reset(self) -> np.ndarray:
        """Reset environment to initial state"""
        self.inventory = 0
        self.cash = 0
        self.position_pnl = 0
        self.mid_price = 100.0
        self.time_step = 0
        return self._get_observation()
    
    def _get_observation(self) -> np.ndarray:
        """Get current state observation"""
        return np.array([
            self.inventory / 1000,  # Normalized
            self.volatility,
            self.spread,
            self.ofi,
            self.time_step / self.max_steps,
            self.position_pnl / 1000
        ], dtype=np.float32)
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        Execute action and return next state
        
        Args:
            action: [bid_offset, ask_offset, aggressiveness]
            
        Returns:
            observation, reward, done, info
        """
        bid_offset, ask_offset, aggressiveness = action
        
        # Simulate market dynamics
        price_change = np.random.randn() * self.volatility
        self.mid_price += price_change
        
        # Update market microstructure
        self.spread = max(0.001, self.spread + np.random.randn() * 0.0001)
        self.ofi = np.clip(np.random.randn() * 0.1, -1, 1)
        
        # Simulate order execution
        bid_price = self.mid_price - self.spread/2 + bid_offset
        ask_price = self.mid_price + self.spread/2 + ask_offset
        
        # Probability of fill based on aggressiveness
        bid_fill_prob = min(0.9, aggressiveness)
        ask_fill_prob = min(0.9, aggressiveness)
        
        # Execute trades
        if np.random.rand() < bid_fill_prob:
            # Buy at bid
            trade_size = np.random.randint(1, 100)
            self.inventory += trade_size
            self.cash -= bid_price * trade_size
        
        if np.random.rand() < ask_fill_prob:
            # Sell at ask
            trade_size = np.random.randint(1, 100)
            self.inventory -= trade_size
            self.cash += ask_price * trade_size
        
        # Calculate reward
        self.position_pnl = self.cash + self.inventory * self.mid_price
        
        reward = (
            self.position_pnl / 1000  # PnL component
            - self.risk_aversion * (self.inventory ** 2)  # Risk penalty
            - self.inventory_penalty * abs(self.inventory)  # Inventory penalty
        )
        
        # Update time
        self.time_step += 1
        done = self.time_step >= self.max_steps
        
        info = {
            'inventory': self.inventory,
            'pnl': self.position_pnl,
            'mid_price': self.mid_price,
            'spread': self.spread
        }
        
        return self._get_observation(), reward, done, info


class DQNAgent:
    """
    Deep Q-Network agent for trading
    Uses experience replay and target networks
    """
    
    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        config: Optional[Dict[str, Any]] = None
    ):
        self.config = config or {}
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Hyperparameters
        self.gamma = config.get('gamma', 0.99)
        self.epsilon = config.get('epsilon_start', 1.0)
        self.epsilon_min = config.get('epsilon_min', 0.01)
        self.epsilon_decay = config.get('epsilon_decay', 0.995)
        self.learning_rate = config.get('learning_rate', 0.001)
        self.batch_size = config.get('batch_size', 64)
        
        # Networks
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.q_network = self._build_network().to(self.device)
        self.target_network = self._build_network().to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        # Experience replay
        self.memory = deque(maxlen=config.get('memory_size', 10000))
    
    def _build_network(self) -> nn.Module:
        """Build Q-network"""
        return nn.Sequential(
            nn.Linear(self.state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, self.action_dim)
        )
    
    def act(self, state: np.ndarray, training: bool = True) -> np.ndarray:
        """
        Select action using epsilon-greedy policy
        
        Args:
            state: Current state
            training: If True, use epsilon-greedy, else greedy
            
        Returns:
            Action vector
        """
        if training and np.random.rand() < self.epsilon:
            # Random action
            return np.random.uniform(-0.01, 0.01, self.action_dim)
        
        # Greedy action
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
        
        # Convert Q-values to continuous action
        action = torch.tanh(q_values).cpu().numpy()[0] * 0.01
        return action
    
    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """Train on batch of experiences"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.FloatTensor(np.array(actions)).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # Current Q values
        current_q = self.q_network(states)
        
        # Target Q values
        with torch.no_grad():
            next_q = self.target_network(next_states)
            target_q = rewards + (1 - dones) * self.gamma * next_q.max(1)[0]
        
        # Loss
        loss = nn.MSELoss()(current_q.max(1)[0], target_q)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # Decay epsilon
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
    
    def update_target_network(self):
        """Update target network weights"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save(self, path: str):
        """Save model"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, path)
        logger.info(f"Model saved to {path}")
    
    def load(self, path: str):
        """Load model"""
        checkpoint = torch.load(path)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']
        logger.info(f"Model loaded from {path}")


class RLMarketMaker:
    """
    RL-based market maker
    Learns optimal bid/ask placement and inventory management
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.env = TradingEnvironment(config)
        self.agent = DQNAgent(
            state_dim=6,
            action_dim=3,
            config=config
        )
        self.episode_rewards: List[float] = []
    
    def train(self, num_episodes: int = 1000):
        """
        Train the RL market maker
        
        Args:
            num_episodes: Number of training episodes
        """
        logger.info(f"Starting training for {num_episodes} episodes")
        
        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                # Select and execute action
                action = self.agent.act(state, training=True)
                next_state, reward, done, info = self.env.step(action)
                
                # Store experience
                self.agent.remember(state, action, reward, next_state, done)
                
                # Train
                self.agent.replay()
                
                state = next_state
                episode_reward += reward
            
            # Update target network
            if episode % 10 == 0:
                self.agent.update_target_network()
            
            self.episode_rewards.append(episode_reward)
            
            # Logging
            if episode % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                logger.info(
                    f"Episode {episode}/{num_episodes} | "
                    f"Avg Reward: {avg_reward:.2f} | "
                    f"Epsilon: {self.agent.epsilon:.3f}"
                )
        
        logger.info("Training complete!")
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:
        """
        Evaluate trained agent
        
        Returns:
            Performance metrics
        """
        total_pnl = 0
        total_trades = 0
        
        for _ in range(num_episodes):
            state = self.env.reset()
            done = False
            episode_pnl = 0
            
            while not done:
                action = self.agent.act(state, training=False)
                state, reward, done, info = self.env.step(action)
                episode_pnl = info['pnl']
            
            total_pnl += episode_pnl
            total_trades += self.env.time_step
        
        return {
            'avg_pnl': total_pnl / num_episodes,
            'avg_trades': total_trades / num_episodes
        }
    
    def get_action(self, state: np.ndarray) -> Dict[str, float]:
        """
        Get market making action for current state
        
        Args:
            state: Current market state
            
        Returns:
            Dict with bid_offset, ask_offset, aggressiveness
        """
        action = self.agent.act(state, training=False)
        return {
            'bid_offset': action[0],
            'ask_offset': action[1],
            'aggressiveness': action[2]
        }


class RLExecutionAgent:
    """
    RL-based execution agent
    Learns optimal order slicing and timing
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # Execution-specific environment
        self.env = TradingEnvironment(config)
        self.agent = DQNAgent(
            state_dim=6,
            action_dim=2,  # [slice_size, aggressiveness]
            config=config
        )
    
    def train(self, num_episodes: int = 1000):
        """Train execution agent"""
        logger.info(f"Training RL Execution Agent for {num_episodes} episodes")
        
        for episode in range(num_episodes):
            state = self.env.reset()
            done = False
            
            while not done:
                action = self.agent.act(state, training=True)
                next_state, reward, done, info = self.env.step(action)
                
                self.agent.remember(state, action, reward, next_state, done)
                self.agent.replay()
                
                state = next_state
            
            if episode % 10 == 0:
                self.agent.update_target_network()
            
            if episode % 100 == 0:
                logger.info(f"Episode {episode}: Epsilon={self.agent.epsilon:.3f}")
    
    def get_optimal_execution(
        self,
        total_quantity: float,
        time_horizon: int,
        market_state: np.ndarray
    ) -> List[Dict[str, Any]]:
        """
        Get optimal execution schedule
        
        Args:
            total_quantity: Total quantity to execute
            time_horizon: Time horizon in steps
            market_state: Current market state
            
        Returns:
            List of execution slices
        """
        remaining = total_quantity
        execution_schedule = []
        
        for t in range(time_horizon):
            action = self.agent.act(market_state, training=False)
            slice_size = min(remaining, abs(action[0]) * total_quantity)
            
            execution_schedule.append({
                'time': t,
                'quantity': slice_size,
                'aggressiveness': action[1]
            })
            
            remaining -= slice_size
            if remaining <= 0:
                break
        
        return execution_schedule


if __name__ == "__main__":
    # Test RL agents
    logger.info("Testing RL Market Maker...")
    
    config = {
        'risk_aversion': 0.01,
        'epsilon_start': 1.0,
        'epsilon_min': 0.01,
        'epsilon_decay': 0.995,
        'learning_rate': 0.001,
        'batch_size': 64
    }
    
    # Train market maker
    mm = RLMarketMaker(config)
    mm.train(num_episodes=500)
    
    # Evaluate
    metrics = mm.evaluate(num_episodes=10)
    logger.info(f"Evaluation Results: {metrics}")
    
    # Test execution agent
    logger.info("\nTesting RL Execution Agent...")
    exec_agent = RLExecutionAgent(config)
    exec_agent.train(num_episodes=500)
    
    # Get optimal execution
    state = np.array([0, 0.01, 0.01, 0, 0, 0])
    schedule = exec_agent.get_optimal_execution(
        total_quantity=10000,
        time_horizon=10,
        market_state=state
    )
    logger.info(f"Optimal execution schedule: {schedule}")
    
    logger.info("\nRL agents tested successfully!")
